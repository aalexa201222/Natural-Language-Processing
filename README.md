# Natural Language Processing

## Overview
This repository contains a series of assignments completed as part of a Natural Language Processing (NLP) course. The labs cover various fundamental and advanced topics in NLP, ranging from text preprocessing and word embeddings to sequence models and advanced neural network architectures. Each lab focuses on a specific aspect of NLP, providing hands-on experience with the techniques and algorithms used in this field.

## Lab 1: Text Preprocessing and Tokenization
In this lab, we explore the fundamental steps of preprocessing text data, including:

- **Tokenization:** Splitting text into words, sentences, or subwords.
- **Normalization:** Converting text to lowercase, removing punctuation, and handling contractions.
- **Stemming and Lemmatization:** Reducing words to their root forms to standardize text data.
- **Stopword Removal:** Filtering out common words that do not contribute significant meaning.
- 
These preprocessing steps are crucial for preparing raw text data for further analysis or modeling.

## Lab 2: Word Embeddings and Vector Representations
This lab focuses on transforming text data into numerical vectors that capture semantic meanings:

- **Bag of Words (BoW):** Simple method representing text as a vector of word counts.
- **TF-IDF:** Enhanced method that considers the importance of words in documents across a corpus.
- **Word2Vec:** Training word embeddings to capture contextual similarity between words.
- **GloVe:** Using pre-trained word vectors to represent words in a continuous vector space.
- 
These techniques form the foundation for many NLP tasks by enabling models to understand and manipulate text data in a mathematical form.

## Lab 3: Sequence Models and Language Modeling
In this lab, we delve into models that handle sequential data, particularly text sequences:

- **N-grams:** Building probabilistic models for sequences of words.
- **Recurrent Neural Networks (RNNs):** Modeling sequences where each input depends on the previous one.
- **Long Short-Term Memory (LSTM):** An advanced RNN variant that overcomes the vanishing gradient problem, making it effective for long sequences.
- **Language Modeling:** Training models to predict the next word in a sequence, a fundamental task for understanding text generation.
- 
Sequence models are essential for tasks like text generation, translation, and sentiment analysis.

## Lab 4: Advanced NLP Models and Applications
This lab covers state-of-the-art models and complex NLP tasks:

- **Transformers:** A model architecture that relies on self-attention mechanisms, enabling parallelization and improving performance on large datasets.
- **BERT (Bidirectional Encoder Representations from Transformers):** Fine-tuning a pre-trained transformer model for specific NLP tasks like classification and question answering.
- **Named Entity Recognition (NER):** Identifying and classifying entities in text.
- **Sentiment Analysis:** Predicting the sentiment behind text data, such as positive, negative, or neutral sentiments.
  
These advanced models are used in a wide range of applications, from chatbots to automated content analysis.

## Installation
Clone the repository to your local machine:
git clone [[https://github.com/aalexa201222/Natural-Language-Processing.git](https://github.com/aalexa201222/Natural-Language-Processing)]

## Contributors
[Andreas Alexandrou](https://www.linkedin.com/in/andreas-alexandrou-056528242) <br />
Sotiris Zenios  <br />
Nicolas Stavrou 
